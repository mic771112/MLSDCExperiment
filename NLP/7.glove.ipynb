{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'jigsaw-toxic-comment-classification-challenge'\n",
    "train = pd.read_csv(f'{base_path}/train.csv')\n",
    "test = pd.read_csv(f'{base_path}/test.csv')\n",
    "submission = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_labels = pd.read_csv(f'{base_path}/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanger_lin/.pyenv/versions/3.6.4/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = f'{base_path}/glove.6B.50d.txt'\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "with open(EMBEDDING_FILE) as f:\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in f)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean()\n",
    "emb_std = all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing (extract useful information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 30000\n",
    "MAX_SENTENSE_LEN = 100\n",
    "EMBEDDING_SIZE = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[\"comment_text\"].values\n",
    "y_train = train[LABELS].values\n",
    "x_test = test[\"comment_text\"].values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_TOKENS)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test)) # is it a proper trick?\n",
    "\n",
    "x_train, x_test = map(tokenizer.texts_to_sequences, [x_train, x_test])\n",
    "x_train, x_test = map(lambda x: sequence.pad_sequences(x, maxlen=MAX_SENTENSE_LEN), [x_train, x_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_TOKENS, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_TOKENS: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding & visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-noise (no drop data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanger_lin/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593/593 [==============================] - 153s 258ms/step - loss: 5.6326 - acc: 0.9034 - val_loss: 4.5540 - val_acc: 0.9952\n",
      "Epoch 2/100\n",
      "593/593 [==============================] - 152s 256ms/step - loss: 3.7951 - acc: 0.9845 - val_loss: 3.1240 - val_acc: 0.9952\n",
      "Epoch 3/100\n",
      "593/593 [==============================] - 152s 256ms/step - loss: 2.6112 - acc: 0.9914 - val_loss: 2.1549 - val_acc: 0.9952\n",
      "Epoch 4/100\n",
      "593/593 [==============================] - 149s 252ms/step - loss: 1.8039 - acc: 0.9927 - val_loss: 1.4912 - val_acc: 0.9952\n",
      "Epoch 5/100\n",
      "593/593 [==============================] - 166s 280ms/step - loss: 1.2503 - acc: 0.9929 - val_loss: 1.0361 - val_acc: 0.9952\n",
      "Epoch 6/100\n",
      "593/593 [==============================] - 161s 272ms/step - loss: 0.8707 - acc: 0.9928 - val_loss: 0.7258 - val_acc: 0.9952\n",
      "Epoch 7/100\n",
      "593/593 [==============================] - 163s 275ms/step - loss: 0.6119 - acc: 0.9922 - val_loss: 0.5143 - val_acc: 0.9952\n",
      "Epoch 8/100\n",
      "593/593 [==============================] - 165s 278ms/step - loss: 0.4363 - acc: 0.9924 - val_loss: 0.3718 - val_acc: 0.9952\n",
      "Epoch 9/100\n",
      "593/593 [==============================] - 177s 299ms/step - loss: 0.3174 - acc: 0.9924 - val_loss: 0.2763 - val_acc: 0.9952\n",
      "Epoch 10/100\n",
      "593/593 [==============================] - 184s 310ms/step - loss: 0.2374 - acc: 0.9920 - val_loss: 0.2124 - val_acc: 0.9952\n",
      "Epoch 11/100\n",
      "593/593 [==============================] - 185s 312ms/step - loss: 0.1831 - acc: 0.9927 - val_loss: 0.1693 - val_acc: 0.9952\n",
      "Epoch 12/100\n",
      "593/593 [==============================] - 189s 319ms/step - loss: 0.1462 - acc: 0.9925 - val_loss: 0.1399 - val_acc: 0.9952\n",
      "Epoch 13/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.1205 - acc: 0.9927 - val_loss: 0.1210 - val_acc: 0.9951\n",
      "Epoch 14/100\n",
      "593/593 [==============================] - 191s 321ms/step - loss: 0.1025 - acc: 0.9927 - val_loss: 0.1057 - val_acc: 0.9952\n",
      "Epoch 15/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.0892 - acc: 0.9928 - val_loss: 0.0974 - val_acc: 0.9952\n",
      "Epoch 16/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.0800 - acc: 0.9923 - val_loss: 0.0896 - val_acc: 0.9951\n",
      "Epoch 17/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.0727 - acc: 0.9918 - val_loss: 0.0842 - val_acc: 0.9952\n",
      "Epoch 18/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.0674 - acc: 0.9930 - val_loss: 0.0820 - val_acc: 0.9951\n",
      "Epoch 19/100\n",
      "593/593 [==============================] - 190s 320ms/step - loss: 0.0633 - acc: 0.9925 - val_loss: 0.0808 - val_acc: 0.9951\n",
      "Epoch 20/100\n",
      "593/593 [==============================] - 177s 298ms/step - loss: 0.0600 - acc: 0.9927 - val_loss: 0.0794 - val_acc: 0.9952\n",
      "Epoch 21/100\n",
      "593/593 [==============================] - 181s 305ms/step - loss: 0.0573 - acc: 0.9916 - val_loss: 0.0807 - val_acc: 0.9951\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN, ))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, mask_zero=True, weights=[embedding_matrix], embeddings_regularizer=l2(1e-5))(model_input)\n",
    "x = keras.layers.GRU(64, return_sequences=True, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n",
    "x = keras.layers.SpatialDropout1D(0.2)(x)\n",
    "x = keras.layers.GRU(32, return_sequences=False, activation=\"relu\", kernel_regularizer=l2(1e-5))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "model_output = keras.layers.Dense(len(LABELS), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "model.compile(keras.optimizers.Adam(3e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 batch_size=256,\n",
    "                 shuffle=True,\n",
    "                 epochs=100, \n",
    "                 validation_split=0.05,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=1, \n",
    "                                                          verbose=1)], \n",
    "                 verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('71.submission.csv', index=False)\n",
    "\n",
    "# ~ 0.9629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanger_lin/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593/593 [==============================] - 298s 502ms/step - loss: 5.6164 - acc: 0.8554 - val_loss: 4.5436 - val_acc: 0.9937\n",
      "Epoch 2/100\n",
      "593/593 [==============================] - 288s 486ms/step - loss: 3.7810 - acc: 0.9905 - val_loss: 3.1088 - val_acc: 0.9949\n",
      "Epoch 3/100\n",
      "593/593 [==============================] - 272s 459ms/step - loss: 2.5952 - acc: 0.9922 - val_loss: 2.1401 - val_acc: 0.9949\n",
      "Epoch 4/100\n",
      "593/593 [==============================] - 282s 476ms/step - loss: 1.7879 - acc: 0.9912 - val_loss: 1.4768 - val_acc: 0.9946\n",
      "Epoch 5/100\n",
      "593/593 [==============================] - 284s 480ms/step - loss: 1.2354 - acc: 0.9929 - val_loss: 1.0226 - val_acc: 0.9951\n",
      "Epoch 6/100\n",
      "593/593 [==============================] - 285s 480ms/step - loss: 0.8572 - acc: 0.9926 - val_loss: 0.7134 - val_acc: 0.9942\n",
      "Epoch 7/100\n",
      "593/593 [==============================] - 285s 481ms/step - loss: 0.5998 - acc: 0.9907 - val_loss: 0.5030 - val_acc: 0.9934\n",
      "Epoch 8/100\n",
      "593/593 [==============================] - 287s 484ms/step - loss: 0.4252 - acc: 0.9924 - val_loss: 0.3622 - val_acc: 0.9930\n",
      "Epoch 9/100\n",
      "593/593 [==============================] - 288s 485ms/step - loss: 0.3078 - acc: 0.9923 - val_loss: 0.2677 - val_acc: 0.9949\n",
      "Epoch 10/100\n",
      "593/593 [==============================] - 286s 482ms/step - loss: 0.2284 - acc: 0.9924 - val_loss: 0.2043 - val_acc: 0.9937\n",
      "Epoch 11/100\n",
      "593/593 [==============================] - 285s 480ms/step - loss: 0.1747 - acc: 0.9921 - val_loss: 0.1603 - val_acc: 0.9944\n",
      "Epoch 12/100\n",
      "593/593 [==============================] - 285s 481ms/step - loss: 0.1381 - acc: 0.9924 - val_loss: 0.1312 - val_acc: 0.9940\n",
      "Epoch 13/100\n",
      "593/593 [==============================] - 264s 444ms/step - loss: 0.1128 - acc: 0.9923 - val_loss: 0.1117 - val_acc: 0.9932\n",
      "Epoch 14/100\n",
      "593/593 [==============================] - 284s 478ms/step - loss: 0.0949 - acc: 0.9923 - val_loss: 0.0975 - val_acc: 0.9941\n",
      "Epoch 15/100\n",
      "593/593 [==============================] - 284s 480ms/step - loss: 0.0819 - acc: 0.9923 - val_loss: 0.0893 - val_acc: 0.9917\n",
      "Epoch 16/100\n",
      "593/593 [==============================] - 287s 485ms/step - loss: 0.0725 - acc: 0.9923 - val_loss: 0.0816 - val_acc: 0.9931\n",
      "Epoch 17/100\n",
      "593/593 [==============================] - 286s 483ms/step - loss: 0.0655 - acc: 0.9923 - val_loss: 0.0790 - val_acc: 0.9942\n",
      "Epoch 18/100\n",
      "593/593 [==============================] - 287s 484ms/step - loss: 0.0602 - acc: 0.9922 - val_loss: 0.0740 - val_acc: 0.9937\n",
      "Epoch 19/100\n",
      "593/593 [==============================] - 286s 483ms/step - loss: 0.0560 - acc: 0.9923 - val_loss: 0.0734 - val_acc: 0.9932\n",
      "Epoch 20/100\n",
      "593/593 [==============================] - 288s 486ms/step - loss: 0.0526 - acc: 0.9923 - val_loss: 0.0724 - val_acc: 0.9919\n",
      "Epoch 21/100\n",
      "593/593 [==============================] - 283s 477ms/step - loss: 0.0497 - acc: 0.9919 - val_loss: 0.0708 - val_acc: 0.9927\n",
      "Epoch 22/100\n",
      "593/593 [==============================] - 261s 441ms/step - loss: 0.0473 - acc: 0.9897 - val_loss: 0.0701 - val_acc: 0.9929\n",
      "Epoch 23/100\n",
      "593/593 [==============================] - 260s 439ms/step - loss: 0.0453 - acc: 0.9907 - val_loss: 0.0717 - val_acc: 0.9917\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN, ))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, mask_zero=True, weights=[embedding_matrix], embeddings_regularizer=l2(1e-5))(model_input)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.SpatialDropout1D(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(32, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "model_output = keras.layers.Dense(len(LABELS), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "model.compile(keras.optimizers.Adam(3e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 batch_size=256,\n",
    "                 shuffle=True,\n",
    "                 epochs=100, \n",
    "                 validation_split=0.05,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=1, \n",
    "                                                          verbose=1)], \n",
    "                 verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('72.submission.csv', index=False)\n",
    "\n",
    "# ~ 0.9701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "593/593 [==============================] - 243s 410ms/step - loss: 0.1103 - acc: 0.8401 - val_loss: 0.0601 - val_acc: 0.9915\n",
      "Epoch 2/100\n",
      "593/593 [==============================] - 246s 414ms/step - loss: 0.0572 - acc: 0.9779 - val_loss: 0.0546 - val_acc: 0.9951\n",
      "Epoch 3/100\n",
      "593/593 [==============================] - 249s 420ms/step - loss: 0.0521 - acc: 0.9856 - val_loss: 0.0526 - val_acc: 0.9895\n",
      "Epoch 4/100\n",
      "593/593 [==============================] - 251s 423ms/step - loss: 0.0487 - acc: 0.9887 - val_loss: 0.0495 - val_acc: 0.9920\n",
      "Epoch 5/100\n",
      "593/593 [==============================] - 249s 420ms/step - loss: 0.0460 - acc: 0.9883 - val_loss: 0.0481 - val_acc: 0.9931\n",
      "Epoch 6/100\n",
      "593/593 [==============================] - 247s 416ms/step - loss: 0.0439 - acc: 0.9826 - val_loss: 0.0472 - val_acc: 0.9880\n",
      "Epoch 7/100\n",
      "593/593 [==============================] - 224s 377ms/step - loss: 0.0421 - acc: 0.9795 - val_loss: 0.0461 - val_acc: 0.9673\n",
      "Epoch 8/100\n",
      "593/593 [==============================] - 234s 394ms/step - loss: 0.0407 - acc: 0.9732 - val_loss: 0.0468 - val_acc: 0.9724\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN, ))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, weights=[embedding_matrix])(model_input)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.SpatialDropout1D(0.1)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(32, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "model_output = keras.layers.Dense(len(LABELS), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "model.compile(keras.optimizers.Adam(3e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 batch_size=256,\n",
    "                 shuffle=True,\n",
    "                 epochs=100, \n",
    "                 validation_split=0.05,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=1, \n",
    "                                                          verbose=1)], \n",
    "                 verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('73.submission.csv', index=False)\n",
    "\n",
    "# ~ 0.9754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4738/4738 [==============================] - 1132s 239ms/step - loss: 0.0665 - acc: 0.9826 - val_loss: 0.0543 - val_acc: 0.9924\n",
      "Epoch 2/100\n",
      "4738/4738 [==============================] - 1069s 226ms/step - loss: 0.0478 - acc: 0.9822 - val_loss: 0.0468 - val_acc: 0.9921\n",
      "Epoch 3/100\n",
      "4738/4738 [==============================] - 1081s 228ms/step - loss: 0.0430 - acc: 0.9661 - val_loss: 0.0459 - val_acc: 0.9160\n",
      "Epoch 4/100\n",
      "4738/4738 [==============================] - 1139s 240ms/step - loss: 0.0400 - acc: 0.9522 - val_loss: 0.0440 - val_acc: 0.9405\n",
      "Epoch 5/100\n",
      "4738/4738 [==============================] - 1554s 328ms/step - loss: 0.0377 - acc: 0.9533 - val_loss: 0.0458 - val_acc: 0.9053\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN, ))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, weights=[embedding_matrix])(model_input)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.SpatialDropout1D(0.1)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(32, return_sequences=True, activation=\"relu\", recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "model_output = keras.layers.Dense(len(LABELS), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "model.compile(keras.optimizers.Adam(3e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 epochs=100, \n",
    "                 validation_split=0.05,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=1, \n",
    "                                                          verbose=1)], \n",
    "                 verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('74.submission.csv', index=False)\n",
    "\n",
    "# ~ 0.97812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "593/593 [==============================] - 155s 262ms/step - loss: 0.0853 - accuracy: 0.9200 - val_loss: 0.0538 - val_accuracy: 0.9952\n",
      "Epoch 2/100\n",
      "593/593 [==============================] - 150s 253ms/step - loss: 0.0502 - accuracy: 0.9545 - val_loss: 0.0496 - val_accuracy: 0.9941\n",
      "Epoch 3/100\n",
      "593/593 [==============================] - 150s 254ms/step - loss: 0.0452 - accuracy: 0.9637 - val_loss: 0.0460 - val_accuracy: 0.9949\n",
      "Epoch 4/100\n",
      "593/593 [==============================] - 149s 251ms/step - loss: 0.0418 - accuracy: 0.9674 - val_loss: 0.0455 - val_accuracy: 0.9945\n",
      "Epoch 5/100\n",
      "593/593 [==============================] - 149s 252ms/step - loss: 0.0394 - accuracy: 0.9589 - val_loss: 0.0449 - val_accuracy: 0.9945\n",
      "Epoch 6/100\n",
      "593/593 [==============================] - 150s 253ms/step - loss: 0.0371 - accuracy: 0.9551 - val_loss: 0.0462 - val_accuracy: 0.9936\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN,))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, weights=[embedding_matrix])(model_input)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(6, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=model_input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train, \n",
    "                 y_train, \n",
    "                 batch_size=256,\n",
    "                 shuffle=True,\n",
    "                 epochs=100, \n",
    "                 validation_split=0.05,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=1, \n",
    "                                                          verbose=1)], \n",
    "                 verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('75.submission.csv', index=False)\n",
    "\n",
    "# ~ 0.97808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4488/4488 [==============================] - 726s 162ms/step - loss: 0.0593 - accuracy: 0.9621 - val_loss: 0.0493 - val_accuracy: 0.9939\n",
      "Epoch 2/100\n",
      "4488/4488 [==============================] - 691s 154ms/step - loss: 0.0442 - accuracy: 0.9841 - val_loss: 0.0459 - val_accuracy: 0.9934\n",
      "Epoch 3/100\n",
      "4488/4488 [==============================] - 703s 157ms/step - loss: 0.0394 - accuracy: 0.9657 - val_loss: 0.0460 - val_accuracy: 0.9939\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_input = keras.Input(shape=(MAX_SENTENSE_LEN,))\n",
    "x = keras.layers.Embedding(MAX_TOKENS, EMBEDDING_SIZE, weights=[embedding_matrix])(model_input)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(6, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=model_input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_split=0.1, \n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=1)], \n",
    "         )\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission[LABELS] = y_pred\n",
    "submission.to_csv('76.submission.csv', index=False)\n",
    "# ~ 0.97846"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection / blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
